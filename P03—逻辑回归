import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv('C:/Users/TT-M/Desktop/ex2data1.txt', names=['exam1', 'exam2', 'admitted'])
# print(data.head())

positive = data[data.admitted.isin(['1'])]  # 1
negetive = data[data.admitted.isin(['0'])]  # 0

fig, ax = plt.subplots(figsize=(6,5))
ax.scatter(positive['exam1'], positive['exam2'], c='b', label='Admitted')
ax.scatter(negetive['exam1'], negetive['exam2'], s=50, c='r', marker='x', label='Not Admitted')
# 设置图例显示在图的上方
box = ax.get_position()
ax.set_position([box.x0, box.y0, box.width , box.height* 0.8])
ax.legend(loc='center left', bbox_to_anchor=(0.2, 1.12),ncol=3)
# 设置横纵坐标名
ax.set_xlabel('Exam 1 Score')
ax.set_ylabel('Exam 2 Score')
plt.show()

#假设函数
def sigmoid(z):
    return 1 / (1 + np.exp(- z))

#代价函数
def cost(theta, X, y):
    first = (-y) * np.log(sigmoid(X @ theta))
    second = (1 - y)*np.log(1 - sigmoid(X @ theta))
    return np.mean(first - second)

#添加列
data.insert(0,'x0',1)

# 设置
X = data.iloc[:, :-1].as_matrix()  # Convert the frame to its Numpy-array representation.
y = data.iloc[:, -1].as_matrix()  # Return is NOT a Numpy-matrix, rather, a Numpy-array.
theta = np.zeros(X.shape[1])
#初始代价函数的值
print(cost(theta,X,y))

#梯度下降函数，只对theta的集合进行一次求偏导，并没有循环
def gradient(theta, X, y):
    return (X.T @ (sigmoid(X @ theta) - y))/len(X)

#使用的是高级优化算法，运行速度通常远远超过梯度下降。方便快捷。
# 只需传入cost函数，已经所求的变量theta，和梯度。
# cost函数定义变量时变量tehta要放在第一个，若cost函数只返回cost，则设置fprime=gradient。
import scipy.optimize as opt
result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))
print(result)
#返回值为数组；迭代次数；

#用最近一次的迭代值来计算
cost(result[0], X, y)

#接下来，我们需要编写一个函数，用我们所学的参数theta来为数据集X输出预测
#当hθ大于等于0.5时，预测 y=1；当hθ小于0.5时，预测 y=0 。
def predict(theta, X):
    probability = sigmoid(X@theta)
    return [1 if x >= 0.5 else 0 for x in probability]  # return a list

#使用这个函数来为分类的精度打分
final_theta = result[0]
predictions = predict(final_theta, X)
correct = [1 if a==b else 0 for (a, b) in zip(predictions, y)]
accuracy = sum(correct) / len(X)
print(accuracy)

#精度打分部分也可以用sklearn库来实现
from sklearn.metrics import classification_report
print(classification_report(predictions, y))

#画出决策边界，
#X×θ=0 (this is the line)
#θ0+x1θ1+x2θ2=0
x1 = np.arange(130, step=0.1)
x2 = -(final_theta[0] + x1*final_theta[1]) / final_theta[2]
fig, ax = plt.subplots(figsize=(8,5))
ax.scatter(positive['exam1'], positive['exam2'], c='b', label='Admitted')
ax.scatter(negetive['exam1'], negetive['exam2'], s=50, c='r', marker='x', label='Not Admitted')
ax.plot(x1, x2)
ax.set_xlim(0, 130)
ax.set_ylim(0, 130)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_title('Decision Boundary')
plt.show()





